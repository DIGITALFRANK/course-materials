\documentclass{beamer}

\usepackage{mjclectureslides}

\renewcommand*{\thefootnote}{\fnsymbol{footnote}}


\title[Eigenvalues and Eigenvectors]{Linear Algebra and Matrix Methods \\
Eigenvalues and Eigenvectors }
%\author[Prof. Michael Carlisle]{Prof. Michael Carlisle}
%\institute{Baruch College, CUNY}
%\date{Spring 2018}
\date{}

%---:----1----:----2----:----3----:----4----:----5----:----6----:----7----:---

\begin{document}

\frame{\titlepage}


\frame{ \frametitle{Eigenvectors, Eigenvalues: ``proper'' directions, scalings}

So far, we've seen ways to solve a system of linear equations, 
\[ A \vec{x} = b. \]
If $A \vec{x} = b$ has no solution, then the least squares (best fit) solution to 
\[ A^t A \hat{x} = A^t b \]
will suffice for ``as close as we can get''. 

\vspace{5mm}

If $A \vec{x} = b$ has a solution, then $b \in C(A)$.

\vspace{5mm}

We are now interested in the special case where $b$ is a scaling of $\vec{x}$. 

}


\frame{ \frametitle{Eigenvectors, Eigenvalues: ``proper'' directions, scalings}

\textbf{Goal}: Solve the $n \times n$ (square) system 
\[ A \vec{x} = \lambda \vec{x} \]
for $\vec{x}$ and $\lambda$. 

\vspace{5mm}

These are considered the ``proper''\footnote{The ``eigen'' in these terms is German for ``proper'' or ``characteristic''.} directions of the matrix $A$, where transformation by the matrix $A$ is equivalent to merely scaling that direction.

}


\frame{ \frametitle{Eigenvectors, Eigenvalues: ``proper'' directions, scalings}

If there are solutions to this \textbf{eigenproblem}\footnote{Some of the examples in this chapter require you to understand arithmetic with complex numbers, i.e. from $\C = \{a + bi \, | \, a, b \in \R, i^2 = -1\}$.} 
\[ A \vec{x} = \lambda \vec{x}, \]
we will call the values in each pair $(\vec{x}, \lambda)$ by the names 
\begin{center}
\textbf{eigenvector} for each $\vec{x} \in \C^n$ 
\end{center}
and 
\begin{center}
\textbf{eigenvalue} for each $\lambda \in \C$. 
\end{center}

}


\frame{ \frametitle{Eigenvectors and eigenvalues via nullspaces}

How can we solve this system? 

\vspace{5mm}

Typically, if you want to solve an equation for a variable, you'll get all instances of that variable on the same side of the equals sign.

\vspace{5mm}

We'll do that here: seeing that scaling a vector by $\lambda$ is the same as multiplying the vector by the scaled identity matrix $\lambda I$, 
\[ A \vec{x} = \lambda \vec{x} \implies A \vec{x} - \lambda \vec{x} = 0 \implies (A - \lambda I)\vec{x} = 0. \]

Thus, if $\vec{x}$ is an eigenvector of $A$, then $\vec{x} \in N(A - \lambda I)$. 

}


\frame{ \frametitle{Eigenvectors and eigenvalues via determinants}

$\vec{x} = 0$ is always a solution. However, the eigenvector $0$ has \emph{any} $\lambda \in \C$ as an eigenvalue; this tells us nothing about the matrix $A$. 

\vspace{5mm}

If a nontrivial $\vec{x}$ solves the eigenproblem, we see that the columns of $A - \lambda I$ are linearly dependent; that is, 

\[ det(A - \lambda I) = 0. \]

}


\frame{ \frametitle{Eigenvectors and eigenvalues via determinants}

\[ det(A - \lambda I) = 0. \]

This is the \textbf{characteristic equation} we need to solve, for $\lambda \in \C$. 

\vspace{5mm}

This also explains why we say $\lambda \in \C$ and $\vec{x} \in \C^n$:
\[ det(A - \lambda I) = 0 \]
is an $n$th degree polynomial equation in $\lambda$, with $n$ roots (with multiplicity) in $\C$.

}


\frame{ \frametitle{Roots of the eigenequation: spectrum of the matrix}

Unfortunately for the desire for a solution to the characteristic equation, we know\footnote{thanks to Neils Abel (1802-1829)} that there is no general method of find roots of polynomials of degree $n \geq 5$.

\vspace{5mm}

That said, we can use various techniques to solve this problem.

}


\frame{ \frametitle{Roots of the eigenequation: spectrum of the matrix}

Define the \textbf{trace} of a square matrix $A$ as the sum of its diagonal: 

\[ tr(A) = \sum_{i=1}^n a_{ii}. \]

\vspace{3mm}

Then, if $\lambda_1, \lambda_2, ..., \lambda_n \in \C$ are the $n$ roots of the characteristic equation of $A$ (i.e. the \textbf{spectrum} of $A$), we have 

\vspace{3mm}

\[ det(A) = \prod_{i=1}^n \lambda_i \,\, \text{ and } \,\, tr(A) = \sum_{i=1}^n \lambda_i. \]

}


\frame{ \frametitle{Roots of the eigenequation: exponentiation}

\begin{itemize}
\item Repeated multiplication by $A$ on an eigenvector repeats scaling by $\lambda$: for any $k \in \N$,

\[ A(A\vec{x}) = A(\lambda \vec{x}) \implies A^2 \vec{x} = \lambda^2 \vec{x} \implies A^k \vec{x} = \lambda^k \vec{x}. \] 

\vspace{3mm}

\item If $A$ is invertible, then all of the eigenvalues of $A$ are nonzero. \\

\vspace{3mm}

If $\vec{x}$ is an eigenvector of $A$ with eigenvalue $\lambda$, \\
then $\vec{x}$ is also an eigenvector of $A^{-1}$, with eigenvalue $\lambda^{-1}$: 

\begin{align*} 
A\vec{x} = \lambda \vec{x} & \implies A^{-1} A\vec{x} = \lambda A^{-1} \vec{x} = \vec{x} \\
 & \implies A^{-1} \vec{x} = \lambda^{-1} \vec{x}.
\end{align*}

\vspace{3mm}

\item If $A$ is singular, then $det(A) = 0$, and $\lambda = 0$ is one of its eigenvalues.
\end{itemize}

}


\frame{ \frametitle{Roots of the eigenequation: triangular, projection}

\begin{itemize}
\item If $A$ is triangular, then $A$'s eigenvalues are the diagonal entries.
\vspace{3mm}
\item If $A$ is symmetric ($A^t = A$), then $A$'s eigenvalues are in $\R$.
\vspace{3mm}
\item If $A = P$ is a projection matrix, then the idempotency of $P$ determines $\lambda$:  since $P^2 = P$, we get 
\[ \lambda^2 \vec{x} = P^2\vec{x} = P\vec{x} = \lambda \vec{x} \vspace{3mm}
\implies \lambda \in \{0,1\}. \]

The eigenvector(s) $\vec{x}$ paired with 
\begin{itemize}
\item $\lambda = 1$ have $P\vec{x}_1 = \vec{x}_1$, and so project to themselves under $P$ ($\vec{x}_1 \in C(P) = C(P^t)$ since $P$ is symmetric); 
\vspace{3mm}
\item $\lambda = 0$ have $P\vec{x}_0 = 0$, and so project to 0 under $P$ ($\vec{x} \in N(P)$).
\end{itemize}
\end{itemize}

}


\frame{ \frametitle{Roots of the eigenequation: orthogonal}

\begin{itemize}
\item If $A = Q$ is orthogonal ($Q^{-1} = Q^t$), then 

\begin{align*} 
Q\vec{x} = \lambda \vec{x} & \implies (Q \vec{x})^t (Q \vec{x}) = (\lambda \vec{x})^t (\lambda \vec{x}) \\
 & \implies \vec{x}^t Q^t Q \vec{x} = \vec{x}^t \vec{x} = ||\vec{x}||^2 = \lambda^2 ||\vec{x}||^2 \\  & \implies \lambda \in \{-1, 1\}.
\end{align*}

\vspace{3mm}

In particular, if $A = R$ is a reflection matrix, $R$ is orthogonal and symmetric, implying $R$ is an involution ($R^2  = I$). Thus, 

\vspace{3mm}

\[ R\vec{x} = \lambda \vec{x} \implies R^2\vec{x} = \lambda^2 \vec{x} = \vec{x} \implies \lambda \in \{-1,1\}. \]

\vspace{3mm}

\item If $A$ is \textbf{skew-symmetric} ($A^t = -A$), then $A$'s eigenvalues are ``pure imaginary'': $\lambda = bi \in \C$ for $b \in \R$. 
\end{itemize}

}


\frame{ \frametitle{Example: 2x2 matrix}

Find the eigenvalues and eigenvectors of $A = \mrr{1}{2}{2}{4}$. 
\[ det(A) = 4-4 = 0 \]

and $A$ is symmetric, so both eigenvalues are real.

\vspace{3mm} 

Thus, $A$ is singular, and so one of $A$'s eigenvalues is 0.

\vspace{5mm}

Find the eigenvalues: 
\begin{align*}
det(A - \lambda I) = 0 & \implies det\mrr{1-\lambda}{2}{2}{4-\lambda} = 0 \\
 & \\
 & \implies (1-\lambda)(4-\lambda) - 4 = 0 \\
 & \\
 & \implies \lambda^2 - 5\lambda = 0 \implies \lambda = 0, 5.
\end{align*}

}


\frame{ \frametitle{Example: 2x2 matrix}

To find the eigenvectors, we need to solve 
\begin{align*}
(A - \lambda I)\vec{x} = 0
\end{align*}
for each $\lambda$. 

\vspace{5mm}

The eigenvectors associated with $\lambda = 0$ are: 
\begin{align*}
A\vec{x} = 0 \implies & \text{ (compute rref(A))} \implies x_1 = -2x_2 \implies \vec{x}_n = x_2\cvv{-2}{1}. 
\end{align*}
The eigenvectors associated with $\lambda = 5$ are: 
\begin{align*}
(A - 5I)\vec{x} = 0 \implies & \mrr{1-5}{2}{2}{4-5} \vec{x} = 0 \\
 \implies \text{ (compute rref(A-5I))} \implies & x_1 = \frac{1}{2}x_2 \implies \vec{x}_n = x_2\cvv{\frac{1}{2}}{1}. 
\end{align*}

}


\frame{ \frametitle{Example: 3x3 matrix}

Find the eigenvalues and eigenvectors of $A = \mrrr{1}{5}{4}{-6}{3}{7}{0}{0}{2}$. 

\vspace{3mm}

Note: $A$ is invertible: 
\[det(A) = 2[1(3) - 5(-6)] = 66 \neq 0. \]
Thus, $A$'s eigenvalues are all nonzero.

\vspace{3mm}

First, find the eigenvalues: 
\begin{align*}
det(A - \lambda I) = 0 & \implies det\mrrr{1-\lambda}{5}{4}{-6}{3-\lambda}{7}{0}{0}{2-\lambda} = 0 \\
 & \implies (2-\lambda)[(1-\lambda)(3-\lambda) + 30] = 0 \\
 & \implies \lambda = 2 \text{ is an eigenvalue.}
\end{align*}

}


\frame{ \frametitle{Example: 3x3 matrix, conjugate pair}

There are two more: factoring out the $2 - \lambda$ term, we have 
\begin{align*}
(1-\lambda)(3-\lambda) + 30 = 0 \implies & 3 - 4\lambda + \lambda^2 + 30 = 0 \\
\implies & \lambda^2 - 4\lambda + 33 = 0 \\
\implies & \lambda = \frac{4 \pm \sqrt{16 - 4(1)(33)}}{2} = 2 \pm i\sqrt{29}. 
\end{align*}
These are the other two eigenvalues.

\vspace{3mm}

Note that they are a conjugate pair of complex numbers. 

}


\frame{ \frametitle{Example: 3x3 matrix, conjugate pair}

Now for the eigenvectors: solving $(A - 2I)\vec{x} = 0$ for $\lambda = 2$ yields 
\begin{align*}
\lambda = 2: & & \vec{x}_n & = x_3 \cvvv{\frac{31}{29}}{-\frac{17}{29}}{1}, \text{ or } x_3 \cvvv{31}{-17}{29}. 
\end{align*}
The conjugate pair of complex eigenvalues have complex conjugate pair eigenvectors: 
\begin{align*}
\lambda = 2 + i \sqrt{29}: & & \vec{x}_n & = x_2 \cvvv{\frac{1}{6}(1 - i\sqrt{29})}{1}{0} \\ 
\lambda = 2 - i \sqrt{29}: & & \vec{x}_n & = x_2 \cvvv{\frac{1}{6}(1 + i\sqrt{29})}{1}{0}. 
\end{align*}

}


\frame{ \frametitle{Example: eigenvalue multiplicity}

Find the eigenvalues and eigenvectors of $A = \mrr{5}{2}{0}{5}$. 

\vspace{5mm}

This one is easy: $A$ is triangular, so clearly the eigenvalues are $\lambda = 5$ with multiplicity 2. 

\vspace{5mm}

What are the eigenvectors? 
\begin{align*}
(A - 5I)\vec{x} = 0 \implies & 2x_2 = 0 \implies \vec{x}_n = x_1 \cvv{1}{0}.
\end{align*}

}


\frame{ \frametitle{Diagonalization of an invertible matrix}

We know an $n \times n$ matrix $A$ has, with multiplicity, $n$ pairs of eigenvectors and eigenvalues. Label them 
\[ (\lambda_1, \vec{x}_1), (\lambda_2, \vec{x}_2), ..., (\lambda_n, \vec{x}_n). \]

If the $n$ eigenvectors of $A$ are \emph{linearly independent}, then we can construct an invertible \textbf{eigenvector matrix} 
\[ X = \left( \vec{x}_1 \,\, \vec{x}_2 \,\, \cdots \,\, \vec{x}_n \right) \]
of the $n$ eigenvectors (which, naturally, are a basis of $\R^n$), and a corresponding diagonal \textbf{eigenvalue matrix} 
\[ \Lambda = \begin{pmatrix}
\lambda_1 & 0 & \cdots & 0 \\
0 & \lambda_2 & \cdots & 0 \\
\vdots & \vdots & \ddots & 0 \\
0 & 0 & \cdots & \lambda_n
\end{pmatrix}. \]
}


\frame{ \frametitle{Diagonalization of an invertible matrix}

Since $A \vec{x}_i = \lambda_i \vec{x}_i$ for all $i=1,2,...,n$, and $X$ is invertible, we have 
\[ AX = X\Lambda \implies X^{-1} A X = \Lambda, \]
or 
\[ A = X \Lambda X^{-1}, \]
called the \textbf{diagonalization} of $A$. 

\vspace{5mm}

Note that the exponentiation property of eigenvalues is easy to see: 
\[ A^n = (X \Lambda X^{-1})^n = (X \Lambda X^{-1})(X \Lambda X^{-1}) \cdots (X \Lambda X^{-1}) = X \Lambda^n X^{-1}, \]
comfirming the notion that the eigenvalues of $A^n$ are $\lambda^n$, with the same eigenvectors as $A$.

}


\frame{ \frametitle{Finding eigenvalues is not a linear operation}

Certain ``shortcuts'' to computing eigenvalues do not work: 

\vspace{5mm}

If $A$ and $B$ are $n \times n$ matrices, with the sets of eigenvalues $\{\lambda_i\}_{i=1}^n$ for $A$ and $\{\beta_i\}_{i=1}^n$ for $B$, then: 

\begin{itemize}
\vspace{3mm}
\item $A+B$ does \emph{not} have the eigenvalues $\lambda_i + \beta_i$ \emph{unless} both $A$ and $B$ are diagonal.
\vspace{3mm}
\item $AB$ does \emph{not} have the eigenvalues $\lambda_i \beta_i$. 
\end{itemize}

}


\frame{ \frametitle{Finding eigenvalues is not a linear operation}

However, 
\vspace{3mm}
\begin{prop}
Assume $A$ and $B$ are diagonalizable, with diagonalizations 
\[ A = X\Lambda X^{-1}, \,\, B = TKT^{-1}. \]
Then 
\[ X = T \iff AB = BA. \]
\end{prop}

}


\frame{ \frametitle{Invertibility vs Diagonalizability?}

If any eigenvalue has multiplicity, then $A$ is only diagonalizable if the \textbf{geometric multiplicity} of each eigenvalue's nullspace, 

\vspace{3mm}

\[ m_G(\lambda) = dim(N(A - \lambda I)), \] 

\vspace{5mm}

equals the \textbf{arithmetic multiplicity} of the eigenvalue, 

\vspace{3mm}

\[ m_A(\lambda) = \text{multiplicity of } \lambda \text{ in } det(A - \lambda I) = 0. \] 

}


\frame{ \frametitle{Invertibility vs Diagonalizability?}

Note that $m_A(\lambda) \geq m_G(\lambda)$ always holds. 

\vspace{5mm}

If all $n$ eigenvalues differ, then for each, 
\[ m_G(\lambda) = m_A(\lambda) = 1, \] 
and $A$ is diagonalizable (even if one of the eigenvalues is 0). 

\vspace{5mm}

If the $n$ eigenvectors of the matrix $A$ are independent, then all eigenvalues differ, and $A$ can be diagonalized. 

}


\frame{ \frametitle{Similar Matrices}

If $A$ is diagonalizable, in form 
\[ A = X \Lambda X^{-1}, \]
there are multiple forms, depending on the scalings and orderings of the eigenvectors in $X$. 

\vspace{5mm}

We will extend the conjugation paradigm to nondiagonalizable $A$. 

\vspace{5mm}

Let $M$ be an invertible matrix. Then we call 
\[ B = M^{-1} A M \] 
\textbf{similar} to $A$. 

}


\frame{ \frametitle{Similar Matrices}

If $A$ is diagonalizable, then $\Lambda$ is similar to $A$. In general, 

\vspace{5mm}

\begin{prop}
$A$ and $M^{-1} A M$ have the same eigenvalues, and if $\vec{x}$ is an eigenvector of $A$, then $M^{-1} \vec{x}$ is an eigenvector of $M^{-1} A M$. 
\end{prop}

\vspace{5mm}

\pf 
\begin{align*} 
A\vec{x} = \lambda \vec{x} \implies (M^{-1} A M)(M^{-1} \vec{x}) & = M^{-1} A \vec{x} \\
 & = M^{-1} \lambda \vec{x} = \lambda (M^{-1} \vec{x}). \,\, \blacksquare 
\end{align*}

}


\frame{ \frametitle{Some similarity-invariant properties}

What properties are \emph{invariant} (do not change) under the transformation $A \mapsto M^{-1} A M$? 

\begin{itemize}
\item eigenvalues
\vspace{3mm}
\item trace
\vspace{3mm}
\item determinant
\vspace{3mm}
\item rank
\vspace{3mm}
\item \# of independent eigenvectors
\vspace{3mm}
\item Jordan form
\end{itemize}

}


\frame{ \frametitle{Some similarity-variant properties}

What properties \emph{do} change under this transformation? 

\begin{itemize}
\item eigenvectors (from $\vec{x}$ to $M^{-1} \vec{x}$)
\vspace{3mm}
\item all four subspaces ($C(A)$ to $C(M^{-1} A M)$, etc.)
\vspace{3mm}
\item singular values (in SVD)
\end{itemize}

}


\frame{ \frametitle{Initial, long-term distributions}

Once again examining powers of $A$, let $c \in \R^n$. \\
Then the eigenvectors of $A$ make a basis of $\R^n$, \\
and so $c$ can be represented via $X$ in the form 
\[ c = \cvvvv{c_1}{c_2}{\ddots}{c_n}, \,\, A = X\Lambda X^{-1} \implies Xc = \sum_{i=1}^n c_i \vec{x}_i. \]
Set $u_0 = Xc$ and define $u_k = A^k u_0$. We can quickly calculate $u_k$: 
\begin{align*}
u_k = A^k u_0 & = A^k (c_1 \vec{x}_1 + c_2 \vec{x}_2 + \cdots + c_n \vec{x}_n) \\
 & = c_1 \lambda_1^k \vec{x}_1 + c_2 \lambda_2^k \vec{x}_2 + \cdots + c_n \lambda_n^k \vec{x}_n.
\end{align*}

}


\frame{ \frametitle{Initial, long-term distributions (Markov matrices)}

Consider a particle $Y_t$ at each time $t$ moving along the states $\{1, 2, ..., n\}$ with one-step probabilities 
\[ p_{ij} = P(Y_{t+1} = i \, | \, Y_{t} = j). \]

\vspace{5mm}

If $A$ is a \textbf{Markov matrix}\footnote{I am leaving out many important probability-based details here.}, whose entries are $a_{ij} = p_{ij}$, then $A^k$ is the matrix of $k$-step probabilities 
\[ P(Y_{t+k} = i \, | \, Y_{t} = j). \]

}


\frame{ \frametitle{Initial, long-term distributions (Markov matrices)}

If we define $u_0$ as the \textbf{initial distribution}\footnote{a probability mass function of the initial position $Y_0$ of the $n$ possible states in the system} of $Y_0$, and define $u_k$ iteratively, as 
\[ u_{k+1} = A u_k, \] 
then the \textbf{long-term distribution} of $Y$ is found by sending $k \to \infty$. 

}


\frame{ \frametitle{Initial, long-term distributions (Markov matrices)}

A property of Markov matrices is that all of their eigenvalues have the property $|\lambda_i| \leq 1$, and, in particular, one of them \emph{is} 1: we label this one $\lambda_1 = 1$. 

\vspace{5mm}

Thus, the \textbf{long-term distribution} of $Y$ is the eigenvector $\vec{x}_1$, as we increase $k$ in the $k$-step distribution $u_k$: recall, if $u_0 = Xc$, then  
\begin{align*}
u_k = c_1 \lambda_1^k \vec{x}_1 + c_2 \lambda_2^k \vec{x}_2 + \cdots + c_n \lambda_n^k \vec{x}_n.
\end{align*}

}


\frame{ \frametitle{Initial, long-term distributions (Markov matrices)}

As $k \to \infty$, $\lambda_i^k \to 0$ for $i > 1$ since $|\lambda_i| < 1$. But $\lambda_1 = 1$, so we get the limit 

\[ u_{\infty} = \lim_{k \to \infty} u_k = \lim_{k \to \infty} c_1 \lambda_1^k \vec{x}_1 + c_2 \lambda_2^k \vec{x}_2 + \cdots + c_n \lambda_n^k \vec{x}_n = c_1 \vec{x}_1. \]

\vspace{3mm}

The scaling of $u_{\infty}$ that yields a probability vector is called $\pi$, the \textbf{long-term}, or \textbf{steady state}, distribution of $Y$.

}


\frame{ \frametitle{Markov matrix example}

Consider a finite-state machine consisting of three states: 1, 2, 3.

\vspace{5mm}

The Markov matrix associated with this machine is 
\[ A = \begin{pmatrix}
0.2 & 0.1 & 0.5 \\ 0.4 & 0.5 & 0.5 \\ 0.4 & 0.4 & 0
\end{pmatrix}. \]

For example, the probability that, from state $X_k = 3$ at time $k$, the machine moves at time $k+1$ to state $X_{k+1} = 2$ is 
\[ p_{23} = P(X_{k+1} = 2 \, | \, X_k = 3) = 0.5. \]
and the probability the machine moves from state $X_k = 2$ to $X_{k+1} = 3$ is 
\[ p_{32} = P(X_{k+1} = 3 \, | \, X_k = 2) = 0.4. \]

}


\frame{ \frametitle{Markov matrix example}

If this machine keeps running, what is the long-term distribution?

\vspace{5mm}

In other words, far out in time, what are the probabilities the machine is in each state?

\vspace{5mm}

We can calculate the long-term state by examining the eigenproblem for $A$.

}


\frame{ \frametitle{Markov matrix example}

The Markov matrix associated with this machine is 
\begin{align*}
A\vec{x} & = \lambda \vec{x} \\
 & \\
\begin{pmatrix} 0.2 & 0.1 & 0.5 \\ 0.4 & 0.5 & 0.5 \\ 0.4 & 0.4 & 0 \end{pmatrix} \vec{x} & = \lambda \vec{x} \\
 & \\
\det \begin{pmatrix} 0.2 - \lambda & 0.1 & 0.5 \\ 0.4 & 0.5 - \lambda & 0.5 \\ 0.4 & 0.4 &  - \lambda \end{pmatrix} & = 0 \\
 & \\
-\lambda^3 + 0.7 \lambda^2 + 0.34 \lambda - 0.04 & = 0.
\end{align*}

}


\frame{ \frametitle{Markov matrix example}

At this point, it would be tedious to compute the roots of the characteristic equation 
\[ -\lambda^3 + 0.7 \lambda^2 + 0.34 \lambda - 0.04 = 0, \]
or, multiplying both sides by -100, 
\[ 100\lambda^3 - 70 \lambda^2 - 34 \lambda + 4 = 0, \]
but since $A$ is a Markov matrix, we can easily see $\lambda_1 = 1$. 

}


\frame{ \frametitle{Markov matrix example}

This simplifies our work significantly; we can use the trace and determinant formulas
\[ \det(A) = \lambda_1 \lambda_2 \lambda_3, \,\,\,\, \tr(A) = \lambda_1 + \lambda_2 + \lambda_3 \]
to get the other two eigenvalues on top of $\lambda_1 = 1$: 

\begin{align*} 
\det(A) & = 0.2(0-0.2) - 0.1(0 - 0.2) + 0.5(0.16 - 0.2) \\
 & = -0.04 + 0.02  - 0.02 = -0.04 \\
\tr(A) & = 0.2 + 0.5 + 0 = 0.7 \\
 & \\
\implies \lambda_2 \lambda_3 & = -0.04, \,\, \lambda_2 + \lambda_3 = -0.3 \implies \lambda_2 = 0.1, \,\, \lambda_3 = -0.4.
\end{align*}

}


\frame{ \frametitle{Markov matrix example}

Thus, our three eigenvalues are 
\[ \lambda_1 = 1, \,\, \lambda_2 = 0.1, \,\, \lambda_3 = -0.4, \]
and we now need the eigenvector $\vec{x_1}$ for $\lambda_1 = 1$. 
\begin{align*} 
(A - I)\vec{x_1} & = 0 \\
 & \implies & \begin{pmatrix} -0.8 & 0.1 & 0.5 \\ 0.4 & -0.5 & 0.5 \\ 0.4 & 0.4 & -1 \end{pmatrix} \vec{x_1} & = 0 \\
 & \implies &  \begin{pmatrix} 1 & 0 & -\frac{5}{6} \\ 0 & 1 & -\frac{5}{3} \\ 0 & 0 & 0 \end{pmatrix} \vec{x_1} & = 0
  \implies \vec{x} = \cvvv{5/6}{5/3}{1} x_3. 
\end{align*}

}


\frame{ \frametitle{Markov matrix example}

As our steady state long-term distribution needs to be a \textbf{probability vector} (a vector of nonnegative values that sum to 1), we choose the appropriate value for $x_3$ to make it so: this is 
\[ \frac{5}{6} + \frac{5}{3} + 1 = \frac{21}{6} \implies \vec{x_1} = \cvvv{5/21}{10/21}{6/21}. \] 

\vspace{5mm}

Note that the other two eigenvectors have terms that sum to 0: 
\[ \vec{x_2} = \cvvv{1}{1/4}{-5/4} x_3, \,\, \vec{x_3} = \cvvv{1}{-1}{0} x_3. \]

}


\frame{ \frametitle{Markov matrix example}

Let's say we start the machine at time 0 with initial distribution 
\[ u_0 = \cvvv{1}{0}{0}, \]
in state 1 with probability 1.

\vspace{5mm}

At time 0, 
\[ u_0 = c_1 \vec{x_1} + c_2 \vec{x_2} + c_3 \vec{x_3} \]
for some constants $c_1$, $c_2$, $c_3$. 

}


\frame{ \frametitle{Markov matrix example}

We can compute $c_1 = 1$, $c_2 = \frac{24}{105}$, $c_3 = \frac{56}{105}$ to get 
\[ u_0 = \cvvv{5/21}{10/21}{6/21} + \frac{24}{105} \cvvv{1}{1/4}{-5/4} + \frac{56}{105} \cvvv{1}{-1}{0} \]

At time $k$, the probabilities of the machine being in each state are 
\begin{align*} 
u_k & = \cvvv{5/21}{10/21}{6/21} + \frac{24}{105} (0.4)^k \cvvv{1}{1/4}{-5/4} + \frac{56}{105} (-0.1)^k \cvvv{1}{-1}{0},
\end{align*}
which, as $k \to \infty$, converges to $\vec{x_1} = \cvvv{5/21}{10/21}{6/21}$.

}


\frame{ \frametitle{The exponential of a square matrix}

We can compute power series-like objects using exponentiation.

\vspace{5mm}

Recall, for any $r, t \in \R$, the natural expoential $e^{rt}$ can be defined as a power series: 
\[ e^{rt} = \sum_{n=0}^{\infty} \frac{1}{n!} (rt)^n = 1 + rt + \frac{1}{2}(rt)^2 + \frac{1}{6}(rt)^3 + \cdots . \]
Likewise, if $A$ is a square matrix and $t \in \R$, we will ignore the scalar convention of writing scalars on the left, and define the matrix exponential by 
\[ e^{At} = \sum_{n=0}^{\infty} \frac{1}{n!} (At)^n = I + At + \frac{1}{2}(At)^2 + \frac{1}{6}(At)^3 + \cdots . \]

}


\frame{ \frametitle{The exponential of a diagonalizable matrix}

If $A$ is diagonalizable, then $A = X \Lambda X^{-1}$ for some eigenvalue matrix $\Lambda$ and eigenvector matrix $X$. 

\vspace{5mm}

Then the exponential is easy to compute: 
\begin{align*} 
e^{At} & = I + At + \frac{1}{2}(At)^2 + \frac{1}{6}(At)^3 + \cdots \\
 & = I + X \Lambda t X^{-1} + \frac{1}{2} (X \Lambda t X^{-1})^2 + \frac{1}{6}(X \Lambda t X^{-1})^3 + \cdots \\
 & = I + X (\Lambda t) X^{-1} + \frac{1}{2} X (\Lambda t)^2 X^{-1} + \frac{1}{6}X (\Lambda t)^3 X^{-1} + \cdots \\
 & = X e^{\Lambda t} X^{-1}. 
\end{align*}
$\therefore$ the eigenvalues of $e^{At}$ are $e^{\Lambda t}$, with the same eigenvectors as $A$.

}


\frame{ \frametitle{The complex exponential as a diagonalizable matrix}

Consider the matrix $A = \mrr{0}{1}{-1}{0}$. 

\vspace{3mm}

Its powers have period 4, and $e^{At}$ is orthogonal: 
\[ A^2 = \mrr{-1}{0}{0}{1}, \,\, A^3 = \mrr{0}{-1}{1}{0}, \,\, A^4 = \mrr{1}{0}{0}{1} = I. \]
Thus, 
\[ e^{At} = \mrr{1 - \frac{1}{2}t^2 + \frac{1}{4}t^4 - \cdots}{t - \frac{1}{3}t^3 + \frac{1}{5}t^5 - \cdots}{-\left(t - \frac{1}{3}t^3 + \frac{1}{5}t^5 - \cdots\right)}{1 - \frac{1}{2}t^2 + \frac{1}{4}t^4 - \cdots}. \]

}


\frame{ \frametitle{The complex exponential as a diagonalizable matrix}

We can rewrite this form of $e^{At}$ in power series terms: 

\begin{align*} 
e^{At} & = \mrr{\sum_{n=0}^{\infty} \frac{(-1)^n t^{2n+1}}{(2n+1)!}}{\sum_{n=0}^{\infty} \frac{(-1)^{n} t^{2n}}{(2n)!}}{-\sum_{n=0}^{\infty} \frac{(-1)^{n} t^{2n}}{(2n)!}}{\sum_{n=0}^{\infty} \frac{(-1)^n t^{2n+1}}{(2n+1)!}} = \mrr{\cos(t)}{\sin(t)}{-\sin(t)}{\cos(t)},
\end{align*}

\vspace{3mm}

the two-dimensional rotation matrix. 

}


\frame{ \frametitle{The complex exponential as a diagonalizable matrix}

The eigenvalue/eigenvector pairs of $A$ are 
\[ (\lambda_1, \vec{x}_1) = \left(i, \cvv{1}{i}\right), \,\, (\lambda_2, \vec{x}_2) = \left(-i, \cvv{1}{-i}\right). \]

The eigenvalue/eigenvector pairs of $e^{At}$ are 
\[ (e^{\lambda_1 t}, \vec{x}_1) = \left(e^{it}, \cvv{1}{i}\right), \,\, (e^{\lambda_2 t}, \vec{x}_2) = \left(e^{-it}, \cvv{1}{-i}\right). \]

Note the connection to \textbf{Euler's formula}: 
\[ e^{it} = \cos(t) + i \sin(t). \]

}


\frame{ \frametitle{Symmetric Matrices}

Recall, a \textbf{symmetric matrix} $A$ is a square matrix such that 

\[ A = A^t. \] 

\vspace{5mm}

If $A$ is symmetric and has independent columns, then $A$ is diagonalizable, and so 

\begin{align*}
A = X \Lambda X^{-1} & \implies A^t = (X \Lambda X^{-1})^t = (X^{-1})^t \Lambda^t X^t = A.
\end{align*}

\vspace{3mm} 

\textbf{Fact}: \,\, $X^{-1} = X^t$, \,\, i.e. $X$ is orthogonal.

}


\frame{ \frametitle{Spectral Theorem}

\begin{thm}
Let $A$ be a real-valued, symmetric matrix. \\
Then its eigenvector matrix $X$ is orthogonal, \\
i.e. $X^t X = I$, and can be chosen as unit length vectors.

\vspace{5mm}

We will relabel this $X$ as $Q$ and write $A = Q \Lambda Q^t$. 
\end{thm}

}


\frame{ \frametitle{Spectral Theorem: Supporting Propositions}

\begin{prop}
$A$ real-valued, symmetric $\implies$ $A$'s eigenvalues are real-valued.
\end{prop}

\vspace{5mm}

\pf Suppose $A \vec{x} = \lambda \vec{x}$. Denote by $\overline{\lambda}$ the complex conjugate of $\lambda$: that is, if $\lambda = a + bi$, then $\overline{\lambda} = a - bi$. 

\vspace{5mm}

We will prove that $\lambda = \overline{\lambda}$, which implies $b = 0$, and so $\lambda = a \in \R$. 

}


\frame{ \frametitle{Spectral Theorem: Supporting Propositions}

\pf (continued) 

\vspace{5mm}

Thus, $A = \overline{A}$ since $A$ is real-valued, and so 
\begin{align*}
A \vec{x} = \lambda \vec{x} & \implies \overline{A \vec{x}} = \overline{\lambda \vec{x}} \implies A \overline{\vec{x}} = \overline{\lambda} \overline{\vec{x}}.
\end{align*}

Transposing, we have 
\begin{align*}
\overline{\vec{x}}^t A^t = \overline{\vec{x}}^t \overline{\lambda} & \implies \overline{\vec{x}}^t A = \overline{\vec{x}}^t \overline{\lambda} \\
 & \implies \overline{\vec{x}}^t A \vec{x} = \overline{\vec{x}}^t \overline{\lambda} \vec{x} \\
 & \implies \overline{\vec{x}}^t \lambda \vec{x} = \overline{\vec{x}}^t \overline{\lambda} \vec{x}  \implies \lambda (\overline{\vec{x}}^t \vec{x}) = \overline{\lambda} (\overline{\vec{x}}^t \vec{x}). 
\end{align*}
But $\overline{\vec{x}}^t \vec{x}$ is a scalar. Thus, $\lambda = \overline{\lambda}$. \,\, $\blacksquare$

}


\frame{ \frametitle{Spectral Theorem: Supporting Propositions}

\begin{prop}
Let $A$ be a real-valued, symmetric matrix. Then, if $\vec{x}_i$ and $\vec{x}_j$ are eigenvectors corresponding to the eigenvalues $\lambda_i$ and $\lambda_j$, $i \neq j$, and $\lambda_i \neq \lambda_j$, then $\vec{x}_i \perp \vec{x}_j$. 
\end{prop}

\vspace{5mm}

\pf \,\, We are given $A \vec{x}_i = \lambda_i \vec{x}_i$, $A \vec{x}_j = \lambda_j \vec{x}_j$, $\lambda_i \neq \lambda_j$, and $A = A^t$. Thus, 
\begin{align*}
\lambda_i (\vec{x}_i^t \vec{x}_j) &  = (\lambda_i \vec{x}_i^t) \vec{x}_j = (\lambda_i \vec{x}_i)^t \vec{x}_j \\
 & = (A \vec{x}_i)^t \vec{x}_j = \vec{x}_i^t A^t \vec{x}_j = \vec{x}_i^t (A \vec{x}_j) \\
 & = \vec{x}_i^t \lambda_j \vec{x}_j = \lambda_j (\vec{x}_i^t \vec{x}_j).
\end{align*}
Therefore, $\lambda_i = \lambda_j$ or $\vec{x}_i^t \vec{x}_j = 0$. As we are given $\lambda_i \neq \lambda_j$, it must then be that $\vec{x}_i^t \vec{x}_j = 0$, i.e. $\vec{x}_i \perp \vec{x}_j$. \,\, $\blacksquare$

}


\frame{ \frametitle{Pivots vs Eigenvalues}

We know that, if $A$ is triangular, then the eigenvalues of $A$ are the diagonal entries, i.e. the pivots, of $A$. 

\vspace{5mm}

However, this is not true for a non-triangular matrix. For a matrix $A = LDU$, the pivots of $A$ correspond to the diagonal entries of $D$, which are \emph{not} the eigenvalues of $A$. 

}


\frame{ \frametitle{Pivots vs Eigenvalues}

We have the following ``compromise'' result: 

\vspace{5mm}

If $d_1, d_2, ..., d_n$ are the diagonal entries of $D$, i.e. the pivots of $A$, and $\lambda_1, \lambda_2, ..., \lambda_n$ are the eigenvalues of $A$, then 
\[ det(A) = \prod_{i=1}^n \lambda_i = \prod_{i=1}^n d_i = det(D). \] 

}



\frame{ \frametitle{Pivots vs Eigenvalues: Symmetric}

If $A$ is symmetric, then the signs of pivots and eigenvalues agree:
\[ \#\text{ positive pivots of } A = A^t = \#\text{ positive eigenvalues of } A = A^t. \]

If all eigenvalues of $A$ are positive (and hence all pivots are positive), then we call $A$ is \textbf{positive definite} matrix. 

\vspace{3mm} 

(We will have another definition of this term shortly.)
 
 

}


\frame{ \frametitle{Schur's Theorem}

\begin{thm}
If $A$ is a square, complex-valued matrix, then $A$ has a decomposition called \textbf{Schur form}, 
\[ A = QTQ^{-1}, \]
where 
\begin{itemize}
\item $Q$ is a \textbf{unitary} matrix, i.e. $Q^{-1} = \overline{Q}^t$, its \textbf{conjugate transpose}, and 
\item $T$ is upper triangular.
\end{itemize}

\vspace{5mm}

If $A$ is a square, real-valued matrix, then the Schur form of $A$ has $Q$ an orthogonal matrix. If $A$ is also symmetric, then $T$ is diagonal.

\end{thm}

}


\frame{ \frametitle{Positive Definite Matrices}

An $n \times n$ matrix $A$ is called \textbf{symmetric positive definite (SPD)} \\
if $A$ is symmetric and satisfies all of the equivalent\footnote{satisfying one satisfies them all!} properties: 
\begin{itemize}
\vspace{3mm}
\item $\vec{x}^t A \vec{x} > 0$ for any $\vec{x} \neq 0 \in \R^n$ (the ``energy'' definition)
\vspace{3mm}
\item all $n$ pivots $d_i > 0$
\vspace{3mm}
\item all $n$ eigenvalues $\lambda_i > 0$
\vspace{3mm}
\item all $n$ upper-left determinants (deleting successive bottom row/column pairs) $> 0$
\vspace{3mm}
\item $A = R^t R$ for some $R$ with independent columns.
\end{itemize}
\vspace{3mm}
If we relax the first (really, any) property to $\geq 0$, any $A$ satisfying is called \textbf{symmetric positive semi-definite (SPSD)}. 

}


\frame{ \frametitle{Positive Definite Matrices: Cholesky decomposition}

If all the inequalities are flipped, we call $A$ \textbf{symmetric negative (semi)definite}.

\vspace{5mm}

If $A$ has positive and negative eigenvalues, we call $A$ \textbf{indefinite}.

\vspace{5mm}

The last property, 

\begin{itemize}
\item $A = R^t R$ for some $R$ with independent columns,
\end{itemize}

is the \textbf{Cholesky decomposition} of $A$; more to the point, 
\[ A = LDU \implies A = LDL^t = (L \sqrt{D})(L \sqrt{D})^t, \]
i.e. $R = (L \sqrt{D})^t$.

}


\frame{ \frametitle{Positive Definite Matrices: two decompositions}

If $A$ is SPD, then there is a decomposition for pivots and another for eigenvalues: 
\[ A = LDL^t \] 
gives the pivots in $D$ (with triangular bookends), and 
\[ A = Q \Lambda Q^t \]
gives the eigenvalues in $\Lambda$ (with orthogonal bookends). 

}


\frame{ \frametitle{Positive Definite Matrices: closed under addition, 2x2}

For once, arithmetic intuition holds: if $A$ and $B$ are SPD, \\
then so is $A+B$: 
\[ \vec{x}^t (A + B) \vec{x} = \vec{x}^t A \vec{x} + \vec{x}^t B \vec{x} > 0. \]
The converse, of course, is not necessarily true.

\vspace{5mm}

A 2x2 matrix $A = \mrr{a}{b}{b}{c}$ is SPD $\iff$ $a > 0$ and $ac - b^2 > 0$. 

}


\frame{ \frametitle{Positive Semidefinite Matrices: Quadratic Forms, Ellipses}

Note the energy definition of an SPSD matrix: 
\begin{itemize}
\item $\vec{x}^t A \vec{x} \geq 0$ for any $\vec{x} \neq 0 \in \R^n$.
\end{itemize}
This kind of function, $f(\vec{x}) = \vec{x}^t A \vec{x}$, is called a \textbf{quadratic form}.

\vspace{5mm}

If $A$ is SPSD, then the equation
\[ \vec{x}^t A \vec{x} = 1 \]
defines an $n$-dimensional ellipsoid in $\R^n$, centered at the origin. 

\vspace{5mm}

In particular, the equation in $\R^2$, 
\begin{align*} 
\vec{x}^t A \vec{x} & = 1, \\
i.e. \,\, \rvv{x}{y}\mrr{a}{b}{b}{c}\cvv{x}{y} & = ax^2 + 2bxy + cy^2 = 1, 
\end{align*}
defines an ellipse. 

}


\frame{ \frametitle{Positive Semidefinite Matrices: principal axis theorem}

This ellipse 
\begin{align*} 
\vec{x}^t A \vec{x} = ax^2 + 2bxy + cy^2 = 1
\end{align*}
is tilted and stretched from the unit circle
\[ \vec{x}^t I \vec{x} = x^2 + y^2 = 1 \]
in that its eigenvectors point in the directions of the minor and major axes, and the eigenvalues determine the lengths of the axes: this is stated in the \textbf{Principal Axis Theorem}.

}


\frame{ \frametitle{Positive Semidefinite Matrices: principal axis theorem}

Factoring the matrix $A = Q \Lambda Q^t$ yields a way to ``standardize'' the ellipse equation in the following way:  
\begin{align*} 
\vec{x}^t A \vec{x} = (\vec{x}^t Q) \Lambda (Q^t \vec{x}) = 1.
\end{align*}

Writing $C = \cvv{c_1}{c_2} = Q^t \vec{x}$, the change into ``ellipse coordinates'', 
\begin{align*} 
\vec{x}^t A \vec{x} = C^t \Lambda C = \lambda_1 c_1^2 + \lambda_2 c_2^2 = 1.
\end{align*}

This implies that, if $\vec{x}_1$ and $\vec{x}_2$ the eigenvectors of $A$, and $\lambda_1 \leq \lambda_2$, then the semi-minor axis is in the directions of $\vec{x}_1$ with length $\frac{1}{\sqrt{\lambda_1}}$, and the semi-major axis is in the direction of $\vec{x}_2$, with length $\frac{1}{\sqrt{\lambda_2}}$.

}


\frame{ \frametitle{Example: principal axis theorem}

Consider the ellipse in the plane defined by 
\[ 10x^2 + 8xy + 7y^2 = 1. \]

\begin{figure}[!ht]
  \centering
    \includegraphics[width=2in]{ellipsewolframalpha.png}
    \caption{$10x^2 + 8xy + 7y^2 = 1$, WolframAlpha}
\end{figure}

}


\frame{ \frametitle{Example: principal axis theorem}

\[ 10x^2 + 8xy + 7y^2 = 1 \]
can be represented by the matrix equation $\vec{x}^t A \vec{x} = 1$: 
\[ \rvv{x}{y} \mrr{10}{4}{4}{7} \cvv{x}{y} = 1. \]

Factoring $A = Q\Lambda Q^t$ yields 
\[ Q = \mrr{0.822}{-0.570}{0.570}{0.822}, \,\, \Lambda = \mrr{12.772}{0}{0}{4.228}. \]

The semi-minor axis has length $0.280$ in the direction $\cvv{0.822}{0.570}$; \\
the semi-major axis is in direction $\cvv{-0.570}{0.822}$ with length $0.486$.

}


\frame{ \frametitle{BONUS: Applications to Ordinary Differential Equations}

Recall that 
\[ \frac{d}{dt} (e^{\lambda t}) = \lambda e^{\lambda t}. \]

\vspace{5mm}

$\frac{d}{dt}$, the \textbf{differential operator} with respect to the variable $t$, can be considered a \emph{matrix} applied to a vector of functions $u$. 

\vspace{5mm}

We will thus examine ODEs via linear algebra.

}


\frame{ \frametitle{Ordinary Differential Equation $\frac{du}{dt} = \lambda u$: $1 \times 1$ case}

The single-variable initial condition ODE 
\[ \frac{du}{dt} = \lambda u, \,\, u(0) = C \text{ with solution } u(t) \]
is solved by the function $u(t) = Ce^{\lambda t}$.

\vspace{5mm}

This is a $1 \times 1$ matrix version of a linear constant coefficient system of ODEs with initial conditions. We will generalize. 

}


\frame{ \frametitle{Ordinary Differential Equation $\frac{du}{dt} = Au$: $n \times n$ case}

The $n \times n$ linear constant coefficient ODE system looks like 

\[ \frac{du}{dt} = A u, \,\, u(0) = \cvvvv{u_1(0)}{u_2(0)}{\vdots}{u_n(0)} \text{ with solution } u(t) = \cvvvv{u_1(t)}{u_2(t)}{\vdots}{u_n(t)} \]

\vspace{3mm}

and is solvable via eigenproblem methods. 

}


\frame{ \frametitle{Ordinary Differential Equation $\frac{du}{dt} = Au$: $n \times n$ case}

If we can solve the eigenproblem 
\[ A \vec{x} = \lambda \vec{x} \]

with $n$ pairs of distinct eigenvalues and independent eigenvectors 
\[ (\lambda_1, \vec{x_1}), (\lambda_2, \vec{x_2}), ..., (\lambda_n, \vec{x_n}), \]

then we can rewrite the initial conditions of the ODE as 
\[ u(0) = \sum_{i=1}^n c_i \vec{x_i} \]
and solve the system with the solution 
\[ u(t) = \sum_{i=1}^n c_i e^{\lambda_i t} \vec{x_i}. \]

}


\frame{ \frametitle{Example: ODE $\frac{du}{dt} = Au$: $n \times n$ case}

Solve the ODE system 
\[ \frac{du}{dt} = \begin{pmatrix}
1 & 1 & 1 \\ 0 & 2 & 1 \\ 0 & 0 & 3
\end{pmatrix} u, \,\, u(0) = \cvvv{6}{5}{4} \text{ for } u(t) = \cvvv{u_1(t)}{u_2(t)}{u_3(t)}. \]

The associated eigenproblem is 
\[ \begin{pmatrix}
1 & 1 & 1 \\ 0 & 2 & 1 \\ 0 & 0 & 3
\end{pmatrix} \vec{x} = \lambda \vec{x}. \]

}


\frame{ \frametitle{Example: ODE $\frac{du}{dt} = Au$: $n \times n$ case}

This triangular system has eigenvalues 
\[ \lambda_1 = 1, \,\, \lambda_2 = 2, \,\, \lambda_3 = 3. \]

The associated eigenvectors are 
\[ \vec{x_1} = \cvvv{1}{0}{0}, \,\, \vec{x_2} = \cvvv{1}{1}{0}, \,\, \vec{x_3} = \cvvv{1}{1}{1}. \]

}


\frame{ \frametitle{Example: ODE $\frac{du}{dt} = Au$: $n \times n$ case}

Solving the initial condition for its coefficients $c_1$, $c_2$, $c_3$ yields 
\begin{align*}
 u(0) & = c_1 \vec{x_1} + c_2 \vec{x_2} + c_3 \vec{x_3} \\
  & \\
 \implies \cvvv{6}{5}{4} & = \begin{pmatrix} 1 & 1 & 1 \\ 0 & 1 & 1 \\ 0 & 0 & 1 \end{pmatrix} \cvvv{c_1}{c_2}{c_3} \\
 & \\
 \implies \cvvv{c_1}{c_2}{c_3} & = \cvvv{1}{1}{4} \\
 & \\
  \implies u(0) & = \vec{x_1} + \vec{x_2} + 4\vec{x_3}.
\end{align*}

}


\frame{ \frametitle{Example: ODE $\frac{du}{dt} = Au$: $n \times n$ case}

Thus, the solution to the ODE system is 
\begin{align*}
 u(t) & = c_1 e^{\lambda_1 t} \vec{x_1} + c_2 e^{\lambda_2 t} \vec{x_2} + c_3 e^{\lambda_3 t} \vec{x_3} \\
 & \\
 & = e^{t} \cvvv{1}{0}{0} + e^{2 t} \cvvv{1}{1}{0} + 4 e^{3 t} \cvvv{1}{1}{1} \\
 & \\
 & = \cvvv{e^t + e^{2t} + 4e^{3t}}{e^{2t} + 4e^{3t}}{4e^{3t}} = \cvvv{u_1(t)}{u_2(t)}{u_3(t)}.
\end{align*}

}


\frame{ \frametitle{Check: ODE $\frac{du}{dt} = Au$: $n \times n$ case}

We check the solution: 

\begin{align*}
 \frac{d}{dt} u(t) & = \frac{d}{dt} \cvvv{e^t + e^{2t} + 4e^{3t}}{e^{2t} + 4e^{3t}}{4e^{3t}} \\
  & \\
  & = \cvvv{e^t + 2e^{2t} + 12e^{3t}}{2e^{2t} + 12e^{3t}}{12e^{3t}} \\
  & \\
  & = \begin{pmatrix} 1 & 1 & 1 \\ 0 & 2 & 1 \\ 0 & 0 & 3 \end{pmatrix} \cvvv{e^t + e^{2t} + 4e^{3t}}{e^{2t} + 4e^{3t}}{4e^{3t}} = Au. \,\, \checkmark
\end{align*}

}


\frame{ \frametitle{Second Order ODEs}

A $1 \times 1$ second order linear ODE has form 
\[ m y'' + by' + ky = 0, \,\, y = y(t), \]
with constants $m, b, k \in \R$. 

\vspace{5mm}

We typically solve this kind of ODE by first assuming the form $y(t) = C e^{\lambda t}$ and checking the coefficient equation 
\[ m \lambda^2 + b\lambda + k = 0, \]
which comes from the derivatives 
\[ y' = \lambda y, \,\, y'' = \lambda^2 y. \]


}


\frame{ \frametitle{Second Order ODEs}

It should be clear that the quadratic equation yields the solutions 
\[ \lambda_1 = \frac{-b + \sqrt{b^2 - 4mk}}{2m}, \,\, \lambda_2 = \frac{-b - \sqrt{b^2 - 4mk}}{2m}. \]

\vspace{5mm}

Then, the solution to the second order ODE is 
\[ y(t) = c_1 e^{\lambda_1 t} + c_2 e^{\lambda_2 t}, \]
where $c_1$ and $c_2$ are given by initial conditions. 

}


\frame{ \frametitle{Second Order ODEs}

How can we use linear algebra to solve a second order linear ODE?

\vspace{5mm}

WLOG we will set $m=1$ to make the calcuations easier.

\vspace{5mm}

Setting $u = \cvv{y}{y'}$, the ODE 
\[ y'' + by' + ky = 0 \]
is transformed into the linear system 
\[ \frac{du}{dt} = Au, \,\, A = \mrr{0}{1}{-k}{-b}. \]


}


\frame{ \frametitle{Second Order ODEs}

We solve the system as before. 

\vspace{5mm}

Note that the characteristic equation of $A$ is precisely the quadratic we used earlier: 
\[ \det(A - \lambda I) = \lambda^2 + b \lambda + k = 0. \]

Thus $\lambda_1$ and $\lambda_2$ are as we calculated, and the eigenvectors are 
\[ \vec{x_1} = \cvv{1}{\lambda_1}, \,\, \vec{x_2} = \cvv{1}{\lambda_2}. \]

}


\frame{ \frametitle{Second Order ODEs}

We get the system solution 
\begin{align*} 
u(t) & = c_1 e^{\lambda_1 t} \vec{x_1} + c_2 e^{\lambda_2 t} \vec{x_2} \\
 & = c_1 e^{\lambda_1 t} \cvv{1}{\lambda_1} + c_2 e^{\lambda_2 t} \cvv{1}{\lambda_2} \\
 & \\
\implies \cvv{y(t)}{y'(t)} & = \cvv{c_1 e^{\lambda_1 t} + c_2 e^{\lambda_2 t}}{c_1 \lambda_1 e^{\lambda_1 t} + c_2 \lambda_2 e^{\lambda_2 t}}.
\end{align*}

\vspace{5mm}

Note that, for this to work, we require $\lambda_1 \neq \lambda_2$. 

\vspace{5mm}

If any eigenvalues match, we need to add solutions of form $Ct^k e^{\lambda t}$. 

}


\frame{ \frametitle{Example: Second Order ODE: General Solution}

Solve for $y = y(t)$: 
\[ y'' - 6y' + 5y = 0, \,\, y(0) = 10, \,\, y'(0) = 4. \]
\begin{align*}
\text{characteristic equation} & & \lambda^2 - 6\lambda + 5 & = 0 \\
\implies & & \lambda_1 = 1, \,\, \lambda_2 & = 5 \\
\implies & & \vec{x_1} = \cvv{1}{1}, \,\, \vec{x_2} & = \cvv{1}{5} \\
\implies & & u(t) = \cvv{y(t)}{y'(t)} & = \cvv{c_1 e^t + c_2 e^{5t}}{c_1 e^t + 5 c_2 e^{5t}}.
\end{align*}

}


\frame{ \frametitle{Example: Second Order ODE: Initial Conditions}

\begin{align*}
\text{initial conditions} & & u(0) = \cvv{y(0)}{y'(0)} & = \cvv{10}{4} \\
\implies & & \cvv{c_1 + c_2}{c_1 + 5c_2} & = \cvv{10}{4} \\
\implies & & c_1 = \frac{23}{2}, \,\, c_2 & = -\frac{3}{2}. \\
\therefore & & y(t) = \frac{23}{2} e^t & - \frac{3}{2} e^{5t}.
\end{align*}

\begin{align*}
\text{ Check: } y'(t) & = \frac{23}{2} e^t - \frac{15}{2} e^{5t}, \,\, y''(t) = \frac{23}{2} e^t - \frac{75}{2} e^{5t} \\
\implies y'' & - 6y' + 5y = 0. \,\, \checkmark
\end{align*}

}


\frame{ \frametitle{Example: Second Order ODE: Matching Eigenvalues}

Solve for $y = y(t)$: 
\[ y'' - 8y' + 16y = 0, \,\, y(0) = 10, \,\, y'(0) = 4. \]
\begin{align*}
\text{char eqn} & & \lambda^2 - 8\lambda + 16 & = 0 \\
\implies & & \lambda_1 = \lambda_2 & = 4 \\
\implies & & \vec{x_1} = \vec{x_2} & = \cvv{1}{4} \\
\implies & & u(t) = \cvv{y(t)}{y'(t)} & = \cvv{(c_1 + c_2 t) e^{4t}}{(4c_1 + c_2 + 4c_2 t) e^{4t}}.
\end{align*}

}


\frame{ \frametitle{Example: Second Order ODE: Matching Eigenvalues}

\begin{align*}
\text{init cond} & & u(0) = \cvv{y(0)}{y'(0)} & = \cvv{10}{4} \\
\implies & & \cvv{c_1}{4c_1 + c_2} & = \cvv{10}{4} \\
\implies & & c_1 = 10, \,\, c_2 & = -36. \\
\therefore & & y(t) & = \left(10 - 36t\right) e^{4t}. \\
& \\
\text{ Check: } & & y'(t) & = (4 - 144t) e^{4t}, \\
 & & y''(t) & = (-128 - 576t) e^{4t} \\
\implies & & y'' - 8y' + 16y & = 0. \,\, \checkmark
\end{align*}

}


\frame{ \frametitle{Long-Term Stability of ODE Solutions via Eigenvalues}

As $t \to \infty$, what happens to $u(t)$? Does the ODE solution: 

\begin{itemize}
\item stabilize ($u(t) \to c \in \R$)? (``vanish" if $c=0$)
\item explode ($|u(t)| \to \infty$)?
\item cycle ($0 < |u(t)| < \infty$ but $u(t)$ continues to vary)?
\end{itemize}

\vspace{5mm}

Since $e^{\lambda t}$ is the structure of the terms of a typical ODE solution, we examine the eigenvalues as complex: $\lambda = r + is \in \C$. 

\begin{itemize}
\item stabilize: \emph{all} eigenvalues have $r < 0$ (exponential decay)
\item explode: \emph{any} eigenvalue has $r > 0$ (exponential growth)
\item cycle: $r = 0$, $s \neq 0$
\end{itemize}

}


\end{document}
